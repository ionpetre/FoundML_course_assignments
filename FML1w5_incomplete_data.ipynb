{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the following sources: \n",
    "\n",
    "https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python (Parul Pandey)\n",
    "https://www.kaggle.com/code/dansbecker/handling-missing-values/notebook (Dan S. Baker)\n",
    "https://www.kaggle.com/code/alexisbcook/missing-values (Alexis B. Cook)\n",
    "https://www.kaggle.com/code/rtatman/data-cleaning-challenge-handling-missing-values (Rachael Tatman)\n",
    "https://www.kaggle.com/code/twinkle0705/a-comprehensive-guide-to-handle-missing-values (Twinkle Khanna)\n",
    "\n",
    "We discuss several methods for handling missing values in datasets. We evaluate each method by training a machine learning model after curating the dataset, and compare their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "Source: https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python\n",
    "\n",
    "# Handling Missing Values in Python\n",
    "\n",
    "![](https://imgur.com/68u0dD2.png)\n",
    "\n",
    ">Real world data often contains a lot of missing values. The main reasons for incomplete data are\n",
    "\n",
    "| Reason for missing Data |  \n",
    "|--|\n",
    "|Data doesn't exist  |  \n",
    "|Data not collected due to human error.  |  \n",
    "|Data deleted accidently  |  \n",
    "|                         |\n",
    "\n",
    "Many machine learning models need the datasets to be complete. We demonstrate in this notebook several methods to handle missing values in a dataset. There are also machine learning models, such as `XGBoost` and `LightGBM` that can handle incomplete datasets (find out in such cases how they do that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import os\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "    \"\"\"Reset the seed for every random library in use (System, numpy)\"\"\"\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "reset_seed(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nice visualization of the missing values in a dataframe\n",
    "# credit: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction. \n",
    "\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to train a classifier and to report a metric on the validation data\n",
    "# Used to evaluate the quality of different imputation methods\n",
    "# After filling in the missing values, we apply this function to see how well the data can be learned\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def score_datasetClassifier(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators = 100, \n",
    "        criterion = 'gini',\n",
    "        max_depth = 5,\n",
    "        min_samples_split = 10,\n",
    "        random_state = 180,\n",
    "        verbose = 1,  \n",
    "        n_jobs = -1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_valid = model.predict(X_valid)\n",
    "    return roc_auc_score(y_valid, preds_valid)#, pos_label='1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Titanic dataset\n",
    "We use the dataset on the passengers on the Titanic: some of their personal data, cabine and ticketing data, and whether they survived or not. The data is highly incomplete, and the goal of this notebook is to test some methods to deal with such data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "titanic_X, titanic_y = fetch_openml(\n",
    "    data_id=40945,\n",
    "    as_frame=True,\n",
    "    return_X_y=True,\n",
    "    parser = 'auto'\n",
    ")\n",
    "\n",
    "print('The Titanic dataset:')\n",
    "print(titanic_X.info())\n",
    "\n",
    "print('\\n The labels:')\n",
    "print(titanic_y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the category features in the dataset with numerical encodings.\n",
    "\n",
    "titanic_X_category_columns = titanic_X.select_dtypes(['category']).columns\n",
    "titanic_X[titanic_X_category_columns] = titanic_X[titanic_X_category_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "print(titanic_X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values in the Titanic dataset\n",
    "\n",
    "titanic_X_missing= missing_values_table(titanic_X)\n",
    "titanic_X_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into train+validation and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(titanic_X_train_valid, titanic_X_test, \n",
    " titanic_y_train_valid, titanic_y_test) = train_test_split(titanic_X,\n",
    "                                                           titanic_y,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=2023,\n",
    "                                                           stratify=titanic_y,\n",
    "                                                           shuffle=True,\n",
    "                                                          )\n",
    "\n",
    "titanic_X_train_valid = titanic_X_train_valid.reset_index(drop=True)\n",
    "titanic_X_test = titanic_X_test.reset_index(drop=True)\n",
    "titanic_y_train_valid = titanic_y_train_valid.reset_index(drop=True)\n",
    "titanic_y_test = titanic_y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Split the training dataset into training and validation\n",
    "\n",
    "(titanic_X_train, titanic_X_valid, \n",
    "titanic_y_train, titanic_y_valid) = train_test_split(titanic_X_train_valid,\n",
    "                                                     titanic_y_train_valid,\n",
    "                                                     test_size=0.25,\n",
    "                                                     random_state=2023,\n",
    "                                                     stratify=titanic_y_train_valid,\n",
    "                                                     shuffle=True,\n",
    "                                                    )\n",
    "\n",
    "del titanic_X_train_valid\n",
    "del titanic_y_train_valid\n",
    "del titanic_X\n",
    "del titanic_y\n",
    "\n",
    "titanic_X_train = titanic_X_train.reset_index(drop=True)\n",
    "titanic_y_train = titanic_y_train.reset_index(drop=True)\n",
    "titanic_X_valid = titanic_X_valid.reset_index(drop=True)\n",
    "titanic_y_valid = titanic_y_valid.reset_index(drop=True)\n",
    "\n",
    "# We merge the X and y to deal easier with the data processing in the two tables. \n",
    "#titanic_train = pd.merge(titanic_X_train, titanic_y_train, left_index=True, right_index=True)\n",
    "#titanic_valid = pd.merge(titanic_X_valid, titanic_y_valid, left_index=True, right_index=True)\n",
    "#titanic_test = pd.merge(titanic_X_test, titanic_y_test, left_index=True, right_index=True)\n",
    "\n",
    "#del titanic_X_train\n",
    "#del titanic_y_train\n",
    "#del titanic_X_valid\n",
    "#del titanic_y_valid\n",
    "#del titanic_X_test\n",
    "#del titanic_y_test\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training samples:', len(titanic_X_train))\n",
    "print(titanic_y_train.value_counts())\n",
    "\n",
    "print('# of validation samples:', len(titanic_X_valid))\n",
    "print(titanic_y_valid.value_counts())\n",
    "\n",
    "print('# of test samples:', len(titanic_X_test))\n",
    "print(titanic_y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Target column \n",
    "\n",
    ">We create a model that predicts whether or not the passengers survived the sinking of the Titanic. Let's examine the distribution of these labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "s = sns.countplot(x = titanic_y_train)\n",
    "sizes=[]\n",
    "for p in s.patches:\n",
    "    height = p.get_height()\n",
    "    sizes.append(height)\n",
    "    s.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(titanic_y_train)*100),\n",
    "            ha=\"center\", fontsize=14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "* 0: Did not Survive while \n",
    "* 1: Survived. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Missing values\n",
    "\n",
    "## Detecting missing values numerically \n",
    "\n",
    ">The first step is to detect the count/percentage of missing values in every column of the dataset. This will give an idea about the distribution of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X_train_missing= missing_values_table(titanic_X_train)\n",
    "titanic_X_train_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting missing data visually using Missingno library\n",
    "\n",
    ">To graphically analyse the missingness of the data, let's use a library called [Missingno](https://github.com/ResidentMario/missingno). It is a package for graphical analysis of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(titanic_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The bar chart above gives a quick graphical overview of the completeness of the dataset. We can see that the age, fare, cabin, embarked, boat, body, and home_dest columns have missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the locations of the missing data \n",
    "\n",
    ">The [msno.matrix](https://github.com/ResidentMario/missingno#matrix) nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(titanic_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The plot has a white horizontal line, for every cell with a missing values. For instance, in the 'embarked' column there is only one instance of missing data, hence one white lines.\n",
    ">\n",
    ">* The sparkline on the right gives an idea of the general shape of the completeness of the data and points out the row with the minimum nullities and the total number of columns in a given dataset, at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons for Missing Values \n",
    "\n",
    ">Before we start treating the missing values ,it is important to understand the various reasons for the missingness in data. Broadly speaking, there can be three possible reasons:\n",
    "\n",
    "Source: Rubin, D. B. (1976). Inference and Missing Data. Biometrika, 63(3), 581â€“592. https://doi.org/10.2307/2335739\n",
    "\n",
    "1. Missing Completely at Random (MCAR)\n",
    ">The missingness on a given variable (Y) are not associated with other variables in a given data set or with the variable (Y) itself. In other words, there is no particular reason for the missing values. \n",
    "\n",
    "2. Missing at Random (MAR)\n",
    ">The missingness on a given variable (Y) is associated with other variables in the data set. For example, males may be less likely to answer to a question on whether they suffer from depression, regardless of the level of their depression.  \n",
    "\n",
    "3. Missing Not at Random (MNAR)\n",
    ">The missingness on a given variable (Y) depend on unobserved data or the value of the missing data itself. For example, people suffering from depression may be less likely to answer to a question on whether they suffer from depression, the higher their level of depression is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the reasons for missing data using matrix plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(titanic_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted by Cabin\n",
    "sorted = titanic_X_train.sort_values('cabin')\n",
    "msno.matrix(sorted)\n",
    "del sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have slightly more data in columns 'Boat' and 'home_dest' when the data on 'cabin' is available. There may be a small correlation here: the passengers in first class may have been better documented and may have gotten places on boats to a larger extent. We can check this by sorting on 'pclass'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted by pclass\n",
    "sorted = titanic_X_train.sort_values('pclass')\n",
    "msno.matrix(sorted)\n",
    "print(sorted['pclass'].value_counts())\n",
    "del sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis seems reinforced by this plot: the passengers in 1st class seem to have better documented data about their cabin. The same for passengers in 1st and 2nd class on their home destination. \n",
    "\n",
    "We can also use a heatmap to check the correlations between the features when it comes to missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(titanic_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap function shows that there are some correlations between cabin and boat and home destination, as observed above. None of the correlations are very strong. Low correlations suggest that the data are MAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating Missing values  \n",
    "\n",
    "\n",
    "![](https://imgur.com/tBvdfyX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Deletion \n",
    "\n",
    ">In Pairwise deletion, only the missing values are deleted. All operations in pandas like mean,sum etc intrinsically skip missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Listwise Deletion/ Dropping rows\n",
    "\n",
    ">During Listwise deletion, complete rows(which contain the missing values) are deleted. As a result, it is also called Complete Case deletion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop all rows where there is at least one missing value in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycopy = titanic_X_train.dropna(axis=0)\n",
    "mycopy.info()\n",
    "del mycopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oops: no data left to analyse!!! This is because every row in our dataset had at least one missing value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dropping complete columns \n",
    "\n",
    "We might have better luck removing all the columns that have at least one missing value instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycopy = titanic_X_train.dropna(axis=1)\n",
    "mycopy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data did we lose?\n",
    "\n",
    "print('Columns in the original training set: ', titanic_X_train.shape[1])\n",
    "print('Columns after dropping N/A: ', mycopy.shape[1])\n",
    "\n",
    "del mycopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a drastic loss. We should rather focus on dropping only the columns where the majority of data is missing. The other columns we deal with via imputations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of data is missing from columns 'cabin', 'boat', 'body', and 'home.dest'. We drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X_train.drop(['cabin', 'boat', 'body', 'home.dest'], axis=1, inplace=True)\n",
    "titanic_X_valid.drop(['cabin', 'boat', 'body', 'home.dest'], axis=1, inplace=True)\n",
    "titanic_X_test.drop(['cabin', 'boat', 'body', 'home.dest'], axis=1, inplace=True)\n",
    "\n",
    "titanic_X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imputate the missing values on 'age' (157) and 'fare' (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputations Techniques\n",
    "\n",
    "![](https://imgur.com/bL0iHde.png)\n",
    "\n",
    ">Imputation refers to replacing missing data with substituted values. This can be done in several different ways, some of them discussed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imputation Techniques\n",
    "  \n",
    "  - Imputating with a constant value\n",
    "  - Imputation using a statistical indicator (mean, median or most frequent) of each column in which the missing values are located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple imputation technique: replace N/A with a constant value, e.g., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace N/A with a constant value\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mycopy_train = titanic_X_train.copy(deep=True)\n",
    "print('Missing data before imputation:\\n ', mycopy_train.isnull().sum())\n",
    "\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value=0) # imputing using constant value\n",
    "constant_imputer.fit(mycopy_train)\n",
    "\n",
    "mycopy_train = pd.DataFrame(\n",
    "    constant_imputer.transform(mycopy_train),\n",
    "    columns = titanic_X_train.columns)\n",
    "\n",
    "#mycopy_train.iloc[:,:] = constant_imputer.transform(mycopy_train)\n",
    "print('\\n No missing data after imputation:\\n', mycopy_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the quality of this imputation technique through how well the new dataset can be learned with a Random Forest Regressor. We will use the same model architecture to evalauate each imputation technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to the validation data\n",
    "\n",
    "mycopy_valid = titanic_X_valid.copy(deep=True)\n",
    "#mycopy_valid.iloc[:,:] = constant_imputer.transform(mycopy_valid)\n",
    "mycopy_valid = pd.DataFrame(\n",
    "    constant_imputer.fit_transform(mycopy_valid),\n",
    "    columns = titanic_X_valid.columns)\n",
    "\n",
    "\n",
    "# Train the RF model and get its score on the validation data\n",
    "\n",
    "print(score_datasetClassifier(\n",
    "    mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "    mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "    titanic_y_train, \n",
    "    titanic_y_valid))\n",
    "\n",
    "del mycopy_train\n",
    "del mycopy_valid\n",
    "\n",
    "# del constant_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another imputation technique: replace N/A with the most frquent value in that feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace N/A with the most frequent value in that feature\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mycopy_train = titanic_X_train.copy(deep=True)\n",
    "print('Missing data before imputation:\\n', mycopy_train.isnull().sum())\n",
    "\n",
    "frequent_imputer = SimpleImputer(strategy='most_frequent') # imputing using constant value\n",
    "frequent_imputer.fit(mycopy_train)\n",
    "mycopy_train = pd.DataFrame(\n",
    "    frequent_imputer.transform(mycopy_train),\n",
    "    columns = titanic_X_train.columns)\n",
    "\n",
    "print('\\n No missing data after imputation:\\n', mycopy_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the quality of this imputation technique through how well the new dataset can be learned with a Random Forest Regressor. We will use the same model architecture to evalauate each imputation technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to the validation data\n",
    "\n",
    "mycopy_valid = titanic_X_valid.copy(deep=True)\n",
    "mycopy_valid = pd.DataFrame(\n",
    "    frequent_imputer.transform(mycopy_valid),\n",
    "    columns = titanic_X_valid.columns)\n",
    "\n",
    "print(score_datasetClassifier(\n",
    "    mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "    mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "    titanic_y_train, \n",
    "    titanic_y_valid))\n",
    "\n",
    "del mycopy_train\n",
    "del mycopy_valid\n",
    "\n",
    "# del frequent_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Imputation Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor Imputation\n",
    ">Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "mycopy_train = titanic_X_train.copy(deep=True)\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=7, weights=\"uniform\")\n",
    "knn_imputer.fit(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']])\n",
    "\n",
    "mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    knn_imputer.transform(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "print('\\n No missing data after imputation:\\n', mycopy_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the quality of this imputation technique through how well the new dataset can be learned with a Random Forest Regressor. We will use the same model architecture to evalauate each imputation technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to the validation data\n",
    "\n",
    "mycopy_valid = titanic_X_valid.copy(deep=True)\n",
    "mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    knn_imputer.transform(mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "# Train the RF model and get its R2 score on the validation data\n",
    "print(score_datasetClassifier(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    titanic_y_train, \n",
    "                    titanic_y_valid))\n",
    "\n",
    "del mycopy_train\n",
    "del mycopy_valid\n",
    "\n",
    "# del knn_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate feature imputation - Multivariate imputation by chained equations (MICE)\n",
    "A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. It performns multiple regressions over random sample ofthe data, then takes the average ofthe multiple regression values and uses that value to impute the missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "mycopy_train = titanic_X_train.copy(deep=True)\n",
    "\n",
    "mice_imputer = IterativeImputer()\n",
    "mice_imputer.fit(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']])\n",
    "\n",
    "mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    mice_imputer.transform(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "print('\\n No missing data after imputation:\\n', mycopy_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the quality of this imputation technique through how well the new dataset can be learned with a Random Forest Regressor. We will use the same model architecture to evalauate each imputation technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to the validation data\n",
    "\n",
    "mycopy_valid = titanic_X_valid.copy(deep=True)\n",
    "\n",
    "mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    mice_imputer.transform(mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "# Train the RF model and get its R2 score on the validation data\n",
    "print(score_datasetClassifier(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    mycopy_valid[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    titanic_y_train, \n",
    "                    titanic_y_valid))\n",
    "\n",
    "del mycopy_train\n",
    "del mycopy_valid\n",
    "\n",
    "# del mice_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that all imputation techniques give roughly the same performance, with the knn- and mice-based imputation just slightly better. We check the result of the knn-based method on the test datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycopy_train = titanic_X_train.copy(deep=True)\n",
    "mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    knn_imputer.transform(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "# Apply the same transformation to the test data\n",
    "\n",
    "mycopy_test = titanic_X_test.copy(deep=True)\n",
    "mycopy_test[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] = pd.DataFrame(\n",
    "    knn_imputer.transform(mycopy_test[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]),\n",
    "    columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "\n",
    "# Train the RF model and get its R2 score on the validation data\n",
    "print(score_datasetClassifier(mycopy_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    mycopy_test[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']], \n",
    "                    titanic_y_train, \n",
    "                    titanic_y_test))\n",
    "\n",
    "del mycopy_train\n",
    "del mycopy_test\n",
    "\n",
    "# del knn_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del constant_imputer\n",
    "del frequent_imputer\n",
    "del knn_imputer\n",
    "del mice_imputer\n",
    "\n",
    "del titanic_X_train\n",
    "del titanic_y_train\n",
    "del titanic_X_valid\n",
    "del titanic_y_valid\n",
    "del titanic_X_test\n",
    "del titanic_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will load a dataset from OpenML and imputate the missing values with different methods. \n",
    "\n",
    "- Load the Ottawa Real Estate DataSet available at https://openml.org/search?type=data&status=active&id=43417&sort=runs. Consider the price as the labels of your dataset. \n",
    "\n",
    ">Q1: What is the data_id needed to load this dataset in your Python code?\n",
    "\n",
    ">Q2: Which of the following features contain missing values: latitude\tlongitude\twalkScore\tpropertyType\tstyle\tyearBuilt\tbedrooms\tbathrooms\tparking\tgarage\tlotDepth\tlotFrontage\tprice? \n",
    "\n",
    "- The price in this dataset is written in US style, with commas separating every block of 3 digits. This will be problematic in your code. Convert the price from its string format to a numerical format. \n",
    "- Check the 'style' feature of your dataset.\n",
    "\n",
    ">Q3 How many possible values does 'style' have in this dataset? \n",
    "\n",
    ">Q4 What is the 2nd largest class in 'style'?\n",
    "- Consider for the rest of this assignment only the data in the largest class in 'style', i.e., \"Detached\". Also, drop the feature 'propertyType' from the dataset. \n",
    "\n",
    ">Q5 How many of the remaining features have missing values? \n",
    "\n",
    ">Q6 What is the feature missing the most values? \n",
    "\n",
    "- Split the data into train, validation, test with the exact same function available in this notebook.\n",
    "- Use the 4 imputation methods demonstrated in this notebook to fill in the missing values. To evaluate the quality of the imputation methods, use the following function (in the same way as demonstrated in this notebook) on your train and validation datasets: \n",
    "\n",
    "def score_datasetRegressor(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators = 100, \n",
    "        criterion = 'squared_error',\n",
    "        max_depth = 10,\n",
    "        min_samples_split = 20,\n",
    "        min_samples_leaf = 5,\n",
    "        random_state = 180,\n",
    "        verbose = 1,  \n",
    "        n_jobs = -1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_valid = model.predict(X_valid)\n",
    "    return r2_score(y_valid, preds_valid)#, pos_label='1')\n",
    "    \n",
    ">Q7 Which imputation method had the best score on the validation dataset? \n",
    "\n",
    ">Q8 What was the score of the best method on the test dataset? Report the result using 2 decimals only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
